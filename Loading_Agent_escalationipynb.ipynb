{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3POQnAyI25Pn",
        "outputId": "cac6a26d-262c-44e2-fa1d-991abe73cdfd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SULZXiH_2_F",
        "outputId": "3a927481-aa86-4275-8813-260339ed2f3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig\n",
        "\n",
        "# Define the path where the model is saved\n",
        "model_path = '/content/drive/My Drive/BERT_SurveySparrow_Model'\n",
        "\n",
        "# Load the config\n",
        "config = BertConfig.from_pretrained(model_path)\n",
        "\n",
        "# Load the model\n",
        "model = BertForSequenceClassification.from_pretrained(model_path, config=config)\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
        "\n",
        "# Move the model to the appropriate device\n",
        "device = torch.device('cpu')\n",
        "model.to(device)\n",
        "\n",
        "print(\"Model loaded successfully\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "def predict_escalation(query):\n",
        "    inputs = tokenizer(query, return_tensors=\"pt\", truncation=True, padding=True, max_length=64)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    prediction = torch.argmax(outputs.logits, dim=1).item()\n",
        "    return \"Escalation needed\" if prediction == 1 else \"No escalation needed\"\n",
        "\n",
        "# Example usage\n",
        "query = \"How to make this work\"\n",
        "result = predict_escalation(query)\n",
        "print(f\"Query: {query}\")\n",
        "print(f\"Prediction: {result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHUi-TyjA6mg",
        "outputId": "8216c836-9ab3-416b-b8d4-bc02f7f15f50"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: How to make this work\n",
            "Prediction: No escalation needed\n"
          ]
        }
      ]
    }
  ]
}